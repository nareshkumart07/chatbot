import streamlit as st
from docx import Document
import pandas as pd
import PyPDF2
from io import StringIO
import os
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from transformers import BartForConditionalGeneration, BartTokenizer
import torch

# --- Model Management (On-Demand Loading) ---

def load_and_cache_model(model_key, loader_func, *args):
    """A generic function to load and cache models in Streamlit's session state."""
    if model_key not in st.session_state:
        with st.spinner(f"Loading {model_key.replace('_', ' ')}... This may take a moment the first time."):
            st.session_state[model_key] = loader_func(*args)
    return st.session_state[model_key]

# --- File Reading and Processing ---

def read_docx(file):
    doc = Document(file)
    return "\n".join([para.text for para in doc.paragraphs])

def read_pdf(file):
    pdf_reader = PyPDF2.PdfReader(file)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text() or ""
    return text

def read_csv(file):
    return pd.read_csv(file).to_string()

def read_txt(file):
    stringio = StringIO(file.getvalue().decode("utf-8"))
    return stringio.read()

def get_file_content(uploaded_file):
    if uploaded_file is not None:
        file_extension = os.path.splitext(uploaded_file.name)[1].lower()
        read_funcs = {".docx": read_docx, ".pdf": read_pdf, ".csv": read_csv, ".txt": read_txt}
        if file_extension in read_funcs:
            return read_funcs[file_extension](uploaded_file)
        st.error("Unsupported file format.")
    return None

# --- RAG Pipeline & Chat Logic ---

def chunk_text(text, chunk_size=500, chunk_overlap=50):
    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size - chunk_overlap)]

def setup_rag_pipeline(text_content):
    encoder_model = load_and_cache_model('encoder_model', SentenceTransformer, 'all-MiniLM-L6-v2')
    chunks = chunk_text(text_content)
    if not chunks:
        st.warning("Could not extract text from the document.")
        return
        
    embeddings = encoder_model.encode(chunks)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(np.array(embeddings))
    st.session_state.chunks = chunks
    st.session_state.faiss_index = index

def ask_query(query):
    """
    Performs RAG to answer a query using the BART model.
    """
    if 'faiss_index' not in st.session_state:
        return "Please upload and process a file first.", None

    encoder_model = st.session_state.encoder_model
    query_emb = encoder_model.encode([query])
    _, I = st.session_state.faiss_index.search(np.array(query_emb), k=3)
    relevant_chunks = [st.session_state.chunks[i] for i in I[0]]
    doc_context = "\n\n---\n\n".join(relevant_chunks)

    try:
        # Load BART model and tokenizer on demand
        bart_tokenizer = load_and_cache_model('bart_tokenizer', BartTokenizer.from_pretrained, "facebook/bart-large-cnn")
        bart_model = load_and_cache_model('bart_model', BartForConditionalGeneration.from_pretrained, "facebook/bart-large-cnn")
        
        # Create a focused input for BART
        input_text = f"Based on the following document context, answer the question: '{query}'\n\nContext:\n{doc_context}"
        inputs = bart_tokenizer(input_text, max_length=1024, return_tensors='pt', truncation=True)
        
        # Generate the answer
        summary_ids = bart_model.generate(**inputs, num_beams=4, max_length=200, min_length=40, early_stopping=True)
        summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        
        response = f"**Answer generated by BART:**\n\n{summary}"
        return response, doc_context

    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        if "CUDA" in str(e):
            st.error("CUDA Error: This can happen on systems with NVIDIA GPUs. Try running in a CPU-only environment.")
        return f"An error occurred while generating the response.", None

# --- Streamlit App UI ---
st.set_page_config(page_title="Chat with your Document (BART)", page_icon="ðŸ“š", layout="wide")

st.markdown("""
<style>
    .stApp { background-color: #FFFFFF; }
    .stButton>button { background-color: #4285F4; color: white; border-radius: 20px; border: 1px solid #4285F4; }
    .stButton>button:hover { background-color: #FFFFFF; color: #4285F4; border: 1px solid #4285F4; }
    blockquote { background-color: #F1F3F4; border-left: 5px solid #4285F4; padding: 10px; margin: 10px 0px; border-radius: 5px; }
</style>
""", unsafe_allow_html=True)

if 'chat_history' not in st.session_state: st.session_state.chat_history = []
if 'all_chats' not in st.session_state: st.session_state.all_chats = []

# --- Sidebar Controls ---
with st.sidebar:
    st.title("ðŸ“„ Document Controls")
    
    uploaded_file = st.file_uploader("Upload your data file", type=["docx", "pdf", "csv", "txt"])
    
    st.info("This chatbot uses the BART model to answer questions based on the content of your uploaded document.", icon="ðŸ’¡")
    
    if uploaded_file:
        if "processed_file" not in st.session_state or st.session_state.processed_file != uploaded_file.name:
            with st.spinner('Reading and indexing your file...'):
                file_content = get_file_content(uploaded_file)
                if file_content:
                    setup_rag_pipeline(file_content)
                    st.session_state.processed_file = uploaded_file.name
                    st.success("File processed successfully!")
                else:
                    st.error("Failed to read or process the file.")

    st.warning("The BART model can take a few minutes to download the first time you use it.", icon="â³")

    st.markdown("---")
    
    col1, col2 = st.columns(2)
    if col1.button("New Chat", use_container_width=True):
        if st.session_state.chat_history:
            st.session_state.all_chats.append(st.session_state.chat_history)
        st.session_state.chat_history = []
        st.rerun()

    if col2.button("Clear Chat", use_container_width=True):
        st.session_state.chat_history = []
        st.rerun()
            
    if st.session_state.all_chats:
        st.markdown("---")
        st.subheader("Chat History")
        for i, chat in enumerate(st.session_state.all_chats):
            preview = chat[0]['content'] if chat and chat[0]['role'] == 'user' else "Empty Chat"
            if st.button(f"Chat {i+1}: {preview[:30]}...", key=f"history_{i}"):
                st.session_state.chat_history = chat

# --- Main Chat Interface ---
st.title("ðŸ’¬ Chat with your Document")
st.caption("Powered by BART")


for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message["role"] == "assistant" and message.get("context"):
            with st.expander("Show Sources"):
                st.markdown(f"> {message['context'].replace('---', '---')}")

if user_query := st.chat_input("Ask a question about your document..."):
    if not uploaded_file:
        st.warning("Please upload a document before asking questions.")
    else:
        st.session_state.chat_history.append({"role": "user", "content": user_query})
        with st.chat_message("user"):
            st.markdown(user_query)

        with st.chat_message("assistant"):
            with st.spinner("Finding answer with BART..."):
                response, context = ask_query(user_query)
                if response:
                    st.markdown(response)
                    st.session_state.chat_history.append({
                        "role": "assistant", 
                        "content": response, 
                        "context": context
                    })

