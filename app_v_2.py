import streamlit as st
from docx import Document
import pandas as pd
import PyPDF2
from io import StringIO
import os
import numpy as np
import faiss
from gpt4all import GPT4All
from sentence_transformers import SentenceTransformer
import google.generativai as genai
# We import these here, but they will only be loaded on demand
from transformers import BartForConditionalGeneration, BartTokenizer
import torch

# --- Model Management (On-Demand Loading) ---

def load_and_cache_model(model_key, loader_func, *args):
    """A generic function to load and cache models in Streamlit's session state."""
    if model_key not in st.session_state:
        with st.spinner(f"Loading {model_key}... This might take a moment."):
            st.session_state[model_key] = loader_func(*args)
    return st.session_state[model_key]

# --- File Reading and Processing ---

def read_docx(file):
    doc = Document(file)
    return "\n".join([para.text for para in doc.paragraphs])

def read_pdf(file):
    pdf_reader = PyPDF2.PdfReader(file)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text() or ""
    return text

def read_csv(file):
    return pd.read_csv(file).to_string()

def read_txt(file):
    stringio = StringIO(file.getvalue().decode("utf-8"))
    return stringio.read()

def get_file_content(uploaded_file):
    if uploaded_file is not None:
        file_extension = os.path.splitext(uploaded_file.name)[1].lower()
        read_funcs = {".docx": read_docx, ".pdf": read_pdf, ".csv": read_csv, ".txt": read_txt}
        if file_extension in read_funcs:
            return read_funcs[file_extension](uploaded_file)
        st.error("Unsupported file format.")
    return None

# --- RAG Pipeline & Chat Logic ---

def chunk_text(text, chunk_size=500, chunk_overlap=50):
    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size - chunk_overlap)]

def setup_rag_pipeline(text_content):
    encoder_model = load_and_cache_model('encoder_model', SentenceTransformer, 'all-MiniLM-L6-v2')
    chunks = chunk_text(text_content)
    if not chunks:
        st.warning("Could not extract text from the document.")
        return
        
    embeddings = encoder_model.encode(chunks)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(np.array(embeddings))
    st.session_state.chunks = chunks
    st.session_state.faiss_index = index

def ask_query(query, model_type, api_key, chat_history):
    if 'faiss_index' not in st.session_state:
        return "Please upload and process a file first.", None

    encoder_model = st.session_state.encoder_model
    query_emb = encoder_model.encode([query])
    _, I = st.session_state.faiss_index.search(np.array(query_emb), k=3)
    relevant_chunks = [st.session_state.chunks[i] for i in I[0]]
    doc_context = "\n\n---\n\n".join(relevant_chunks)

    history_str = ""
    recent_history = chat_history[-4:]
    if recent_history:
        history_str += "Here is the recent conversation history:\n"
        for msg in recent_history:
            history_str += f"{msg['role'].capitalize()}: {msg['content']}\n"

    prompt = f"""
You are a helpful AI assistant. Answer the user's question based on the conversation history and the document context.
{history_str}
Document Context:
---
{doc_context}
---
User's Latest Question: {query}
Answer:
"""
    try:
        if model_type == 'Fast Model (Gemini)':
            if not api_key:
                return "Error: Please enter your Google AI API key.", None
            genai.configure(api_key=api_key)
            gen_model = genai.GenerativeModel("gemini-pro")
            response = gen_model.generate_content(prompt)
            return response.text, doc_context
            
        elif model_type == 'Advanced Model (BART)':
            bart_tokenizer = load_and_cache_model('bart_tokenizer', BartTokenizer.from_pretrained, "facebook/bart-large-cnn")
            bart_model = load_and_cache_model('bart_model', BartForConditionalGeneration.from_pretrained, "facebook/bart-large-cnn")
            
            # Create a more focused input for BART
            input_text = f"Based on the following document context, answer the question: '{query}'\n\nContext:\n{doc_context}"
            inputs = bart_tokenizer(input_text, max_length=1024, return_tensors='pt', truncation=True)
            
            # **FIX:** Pass the entire 'inputs' dictionary to the generate method
            summary_ids = bart_model.generate(**inputs, num_beams=4, max_length=200, min_length=40, early_stopping=True)
            summary = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            
            response = f"**Answer generated by BART:**\n\n{summary}"
            return response, doc_context

        else: # Normal Model (Local)
            llm_model = load_and_cache_model('llm_model', GPT4All, "orca-mini-3b-gguf2-q4_0.gguf")
            response = llm_model.generate(prompt, max_tokens=512)
            return response, doc_context
            
    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
        # Check for common CUDA errors if torch is involved
        if "CUDA" in str(e):
            st.error("CUDA Error: Please ensure you have a compatible NVIDIA GPU and properly configured PyTorch with CUDA support, or use a CPU-only environment.")
        return f"An error occurred while generating the response.", None

# --- Streamlit App UI ---
st.set_page_config(page_title="Chat with your Data", page_icon="ðŸ’¬", layout="wide")

st.markdown("""
<style>
    .stApp { background-color: #FFFFFF; }
    .stButton>button { background-color: #4285F4; color: white; border-radius: 20px; border: 1px solid #4285F4; }
    .stButton>button:hover { background-color: #FFFFFF; color: #4285F4; border: 1px solid #4285F4; }
    blockquote { background-color: #F1F3F4; border-left: 5px solid #4285F4; padding: 10px; margin: 10px 0px; border-radius: 5px; }
</style>
""", unsafe_allow_html=True)

if 'chat_history' not in st.session_state: st.session_state.chat_history = []
if 'all_chats' not in st.session_state: st.session_state.all_chats = []

# --- Sidebar Controls ---
with st.sidebar:
    st.title("ðŸ“„ Chat Controls")
    
    uploaded_file = st.file_uploader("Upload your data file", type=["docx", "pdf", "csv", "txt"])
    
    st.info("Note: The chatbot's knowledge is limited to the uploaded document.", icon="ðŸ’¡")
    
    if uploaded_file:
        if "processed_file" not in st.session_state or st.session_state.processed_file != uploaded_file.name:
            with st.spinner('Reading and indexing your file...'):
                file_content = get_file_content(uploaded_file)
                if file_content:
                    setup_rag_pipeline(file_content)
                    st.session_state.processed_file = uploaded_file.name
                    st.success("File processed successfully!")
                else:
                    st.error("Failed to read or process the file.")

    model_choice = st.selectbox(
        "Choose a model:",
        ('Normal Model (Local)', 'Fast Model (Gemini)', 'Advanced Model (BART)'),
        help="Models will be downloaded the first time you select them."
    )
    
    api_key = ""
    if model_choice == 'Fast Model (Gemini)':
        api_key = st.text_input("Enter Google AI API Key", type="password")
        st.markdown("For using the faster model you need to paste your Gemini API key here.")
    
    st.warning("Be patient: The 'Local' and 'BART' models can take a few minutes to download the first time you use them.", icon="â³")

    st.markdown("---")
    
    col1, col2 = st.columns(2)
    if col1.button("New Chat", use_container_width=True):
        if st.session_state.chat_history:
            st.session_state.all_chats.append(st.session_state.chat_history)
        st.session_state.chat_history = []
        st.rerun()

    if col2.button("Clear Chat", use_container_width=True):
        st.session_state.chat_history = []
        st.rerun()
            
    if st.session_state.all_chats:
        st.markdown("---")
        st.subheader("Chat History")
        for i, chat in enumerate(st.session_state.all_chats):
            preview = chat[0]['content'] if chat and chat[0]['role'] == 'user' else "Empty Chat"
            if st.button(f"Chat {i+1}: {preview[:30]}...", key=f"history_{i}"):
                st.session_state.chat_history = chat

# --- Main Chat Interface ---
st.title("ðŸ’¬ Chatbot")

for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message["role"] == "assistant" and message.get("context"):
            with st.expander("Show Sources"):
                st.markdown(f"> {message['context'].replace('---', '---')}")

if user_query := st.chat_input("Ask a question about your document..."):
    if not uploaded_file:
        st.warning("Please upload a document before asking questions.")
    else:
        st.session_state.chat_history.append({"role": "user", "content": user_query})
        with st.chat_message("user"):
            st.markdown(user_query)

        with st.chat_message("assistant"):
            # The spinner is now inside the model loading function, 
            # so we just need a general one here.
            with st.spinner("Thinking..."):
                response, context = ask_query(
                    user_query, model_choice, api_key, st.session_state.chat_history
                )
                if response:
                    st.markdown(response)
                    st.session_state.chat_history.append({
                        "role": "assistant", 
                        "content": response, 
                        "context": context
                    })
